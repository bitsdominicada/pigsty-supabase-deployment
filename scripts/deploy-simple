#!/usr/bin/env bash
#==============================================================#
# File      :   deploy-simple
# Desc      :   Simplified deployment using official Pigsty flow
# Path      :   scripts/deploy-simple
# Usage     :   ./scripts/deploy-simple all
# Copyright :   2025 - Automation Script
# License   :   AGPLv3
#==============================================================#
# Supports both SSH key (professional) and password (legacy) auth
# SSH key mode: Set SSH_USER in .env (auto-detects key)
# Password mode: Set VPS_ROOT_PASSWORD in .env
#==============================================================#

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"

source "${SCRIPT_DIR}/utils.sh"

#--------------------------------------------------------------#
# Generate pigsty.yml using official ./configure + inject vars
#--------------------------------------------------------------#
generate_pigsty_config() {
    log_step "Generating pigsty.yml with official Pigsty configure"

    load_env

    # Copy .env to VPS temporarily (will be used by inject-vars.py)
    log_info "Copying .env to VPS..."
    scp_deploy "${PROJECT_ROOT}/.env" "/tmp/.env.pigsty"

    # Copy inject-vars.py script to VPS
    log_info "Copying inject-vars.py to VPS..."
    scp_deploy "${PROJECT_ROOT}/lib/inject-vars.py" "/tmp/inject-vars.py"

    # Run ./configure on VPS to generate base pigsty.yml
    log_info "Running ./configure -c app/supa on VPS..."
    ssh_exec "cd ~/pigsty && ./configure -c app/supa -i ${VPS_HOST} -n"

    log_success "Base pigsty.yml generated by Pigsty configure"

    # Inject our variables into pigsty.yml
    log_info "Injecting custom variables into pigsty.yml..."
    ssh_exec "python3 /tmp/inject-vars.py ~/pigsty/pigsty.yml /tmp/.env.pigsty"

    log_success "Variables injected into pigsty.yml"

    # Display configuration summary
    echo ""
    log_info "Configuration summary:"
    echo "  Domain:      ${SUPABASE_DOMAIN:-not set}"
    echo "  VPS IP:      ${VPS_HOST}"
    echo "  SSL Email:   ${LETSENCRYPT_EMAIL:-not set}"
    echo "  SSL Enabled: ${USE_LETSENCRYPT:-false}"
    echo "  B2 Bucket:   ${S3_BUCKET:-not set}"
    echo "  B2 Endpoint: ${S3_ENDPOINT:-not set}"
    echo ""

    # Cleanup temporary files from VPS
    log_info "Cleaning up temporary files..."
    ssh_exec "rm -f /tmp/.env.pigsty /tmp/inject-vars.py"

    log_success "pigsty.yml configured successfully"
}

#--------------------------------------------------------------#
# Prepare VPS (same as before)
#--------------------------------------------------------------#
prepare_vps() {
    log_step "Preparing VPS"
    "${SCRIPT_DIR}/modules/01-prepare.sh"
}

#--------------------------------------------------------------#
# Download Pigsty
#--------------------------------------------------------------#
download_pigsty() {
    log_step "Downloading Pigsty"

    load_env

    log_info "Installing Pigsty v3.6.1 on VPS..."

    ssh_exec "curl -fsSL https://repo.pigsty.io/get | bash"

    log_success "Pigsty downloaded and extracted"
}

#--------------------------------------------------------------#
# Update Supabase container versions to latest
#--------------------------------------------------------------#
update_supabase_versions() {
    load_env
    log_step "Updating Supabase container versions"

    log_info "Patching ~/pigsty/app/supabase/docker-compose.yml with latest versions..."

    # Define version updates (Pigsty -> Official latest)
    # These are the versions from Supabase official docker-compose.yml as of 2025-11
    ssh_exec "cd ~/pigsty/app/supabase && \
        # Studio: 2025.06.16 -> 2025.11.10
        sed -i 's|supabase/studio:2025.06.16-sha-c4316c3|supabase/studio:2025.11.10-sha-5291fe3|g' docker-compose.yml && \
        # Auth (GoTrue): v2.174.0 -> v2.182.1
        sed -i 's|supabase/gotrue:v2.174.0|supabase/gotrue:v2.182.1|g' docker-compose.yml && \
        # PostgREST: v12.2.12 -> v13.0.7
        sed -i 's|postgrest/postgrest:v12.2.12|postgrest/postgrest:v13.0.7|g' docker-compose.yml && \
        # Realtime: v2.34.47 -> v2.63.0
        sed -i 's|supabase/realtime:v2.34.47|supabase/realtime:v2.63.0|g' docker-compose.yml && \
        # Storage: v1.23.0 -> v1.29.0
        sed -i 's|supabase/storage-api:v1.23.0|supabase/storage-api:v1.29.0|g' docker-compose.yml && \
        # Meta: v0.89.3 -> v0.93.1
        sed -i 's|supabase/postgres-meta:v0.89.3|supabase/postgres-meta:v0.93.1|g' docker-compose.yml && \
        # Edge Functions: v1.67.4 -> v1.69.23
        sed -i 's|supabase/edge-runtime:v1.67.4|supabase/edge-runtime:v1.69.23|g' docker-compose.yml && \
        # Analytics (Logflare): 1.15.4 -> 1.22.6
        sed -i 's|supabase/logflare:1.15.4|supabase/logflare:1.22.6|g' docker-compose.yml && \
        echo 'Versions updated successfully'

        # Add PG_META_CRYPTO_KEY to studio and meta services if not present
        if ! grep -q 'PG_META_CRYPTO_KEY' docker-compose.yml; then
            # Add to studio service (after POSTGRES_PASSWORD line)
            sed -i '/POSTGRES_PASSWORD.*POSTGRES_PASSWORD/a\\      PG_META_CRYPTO_KEY: \${PG_META_CRYPTO_KEY}' docker-compose.yml
            # Add to meta service (after PG_META_DB_PASSWORD line)
            sed -i '/PG_META_DB_PASSWORD/a\\      CRYPTO_KEY: \${PG_META_CRYPTO_KEY}' docker-compose.yml
            echo 'Added PG_META_CRYPTO_KEY to docker-compose.yml'
        fi

        # Add missing POSTGRES_* vars to Studio (required for newer versions)
        # Fix for: Failed to retrieve migration history / Zod validation error
        if ! grep -q 'POSTGRES_HOST.*POSTGRES_HOST' docker-compose.yml; then
            sed -i '/POSTGRES_PASSWORD: \\\${POSTGRES_PASSWORD}/a\\      POSTGRES_HOST: \${POSTGRES_HOST}\n      POSTGRES_PORT: \${POSTGRES_PORT}\n      POSTGRES_DB: \${POSTGRES_DB}' docker-compose.yml
            echo 'Added POSTGRES_HOST/PORT/DB to Studio'
        fi

        # Add SMTP configuration to auth service (GoTrue)
        if ! grep -q 'SMTP_HOST' docker-compose.yml; then
            # Find the auth service and add SMTP vars after GOTRUE_EXTERNAL_EMAIL_ENABLED
            sed -i '/GOTRUE_EXTERNAL_EMAIL_ENABLED/a\\      SMTP_HOST: \${SMTP_HOST}\n      SMTP_PORT: \${SMTP_PORT}\n      SMTP_USER: \${SMTP_USER}\n      SMTP_PASS: \${SMTP_PASSWORD}\n      SMTP_SENDER_NAME: \${SMTP_SENDER_NAME}\n      SMTP_ADMIN_EMAIL: \${SMTP_ADMIN_EMAIL}' docker-compose.yml
            echo 'Added SMTP configuration to auth service'
        fi

        # Add port mapping for Studio (3001:3000) since Grafana uses port 3000
        if ! grep -q '3001:3000' docker-compose.yml; then
            sed -i '/container_name: supabase-studio/a\\    ports:\\n      - \"3001:3000\"' docker-compose.yml
            echo 'Added port 3001:3000 mapping to Studio'
        fi"

    log_success "Container versions updated to latest"
    log_info "Updated versions:"
    echo "  studio:    2025.11.10-sha-5291fe3"
    echo "  gotrue:    v2.182.1"
    echo "  postgrest: v13.0.7"
    echo "  realtime:  v2.63.0"
    echo "  storage:   v1.29.0"
    echo "  meta:      v0.93.1"
    echo "  functions: v1.69.23"
    echo "  logflare:  1.22.6"
}

#--------------------------------------------------------------#
# Bootstrap Ansible
#--------------------------------------------------------------#
bootstrap_ansible() {
    load_env
    log_step "Bootstrapping Ansible"

    log_info "Installing Ansible and dependencies..."

    ssh_exec "cd ~/pigsty && ./bootstrap"

    log_success "Ansible bootstrapped"
}

#--------------------------------------------------------------#
# Install Pigsty Stack (PostgreSQL + MinIO + Infra)
#--------------------------------------------------------------#
install_pigsty() {
    load_env
    log_step "Installing Pigsty Stack"

    log_info "This will take 10-20 minutes..."
    log_info "Installing: PostgreSQL 17, Patroni, MinIO, Grafana, Prometheus..."

    ssh_exec "cd ~/pigsty && ./install.yml"

    log_success "Pigsty stack installed"
    log_info "PostgreSQL 17 with Patroni: ✓"
    log_info "MinIO S3 storage: ✓"
    log_info "Monitoring (Grafana, Prometheus): ✓"

    if [ "${USE_LETSENCRYPT:-false}" = "true" ]; then
        log_info "SSL Certificate: ✓ (Let's Encrypt)"
        log_info "Domain: https://${SUPABASE_DOMAIN}"
    fi
}

#--------------------------------------------------------------#
# Install Docker
#--------------------------------------------------------------#
install_docker() {
    load_env
    log_step "Installing Docker"

    log_info "Installing Docker and Docker Compose..."

    ssh_exec "cd ~/pigsty && ./docker.yml"

    # Ensure Docker daemon is running (docker.yml installs but may not start it)
    log_info "Ensuring Docker daemon is running..."
    ssh_exec "sudo systemctl enable docker && sudo systemctl start docker && sleep 3"

    # Verify Docker is working
    ssh_exec "sudo docker ps" || {
        log_error "Docker daemon failed to start"
        exit 1
    }

    log_success "Docker installed and running"
}

#--------------------------------------------------------------#
# Fix Vault functions to use pgsodium server key
#--------------------------------------------------------------#
fix_vault_pgsodium() {
    load_env
    log_step "Fixing Vault to use pgsodium server key"

    log_info "Patching vault functions to use pgsodium native encryption..."

    # The supabase_vault extension has its own C functions that don't share
    # the server key with pgsodium. We replace them with pgsodium native functions.

    # Create SQL patch file on VPS
    ssh_exec "cat > /tmp/vault_fix.sql << 'EOF'
-- Fix vault.create_secret to use pgsodium native function
CREATE OR REPLACE FUNCTION vault.create_secret(new_secret text, new_name text DEFAULT NULL::text, new_description text DEFAULT ''::text, new_key_id uuid DEFAULT NULL::uuid)
 RETURNS uuid
 LANGUAGE plpgsql
 SECURITY DEFINER
 SET search_path TO ''
AS \$function\$
DECLARE
  rec record;
BEGIN
  INSERT INTO vault.secrets (secret, name, description)
  VALUES (new_secret, new_name, new_description)
  RETURNING * INTO rec;
  UPDATE vault.secrets s
  SET secret = encode(pgsodium.crypto_aead_det_encrypt(
    message := convert_to(rec.secret, 'utf8'),
    additional := convert_to(s.id::text, 'utf8'),
    key_id := 0::bigint,
    context := 'pgsodium'::bytea,
    nonce := rec.nonce
  ), 'base64')
  WHERE id = rec.id;
  RETURN rec.id;
END
\$function\$;

-- Fix vault.decrypted_secrets view to use pgsodium native function
CREATE OR REPLACE VIEW vault.decrypted_secrets AS
 SELECT id, name, description, secret,
    convert_from(pgsodium.crypto_aead_det_decrypt(
        message => decode(secret, 'base64'::text),
        additional => convert_to(id::text, 'utf8'::name),
        key_id => 0::bigint,
        context => '\x7067736f6469756d'::bytea,
        nonce => nonce
    ), 'utf8'::name) AS decrypted_secret,
    key_id, nonce, created_at, updated_at
   FROM vault.secrets s;

-- Fix vault.update_secret to use pgsodium native function
CREATE OR REPLACE FUNCTION vault.update_secret(secret_id uuid, new_secret text DEFAULT NULL::text, new_name text DEFAULT NULL::text, new_description text DEFAULT NULL::text, new_key_id uuid DEFAULT NULL::uuid)
 RETURNS void
 LANGUAGE plpgsql
 SECURITY DEFINER
 SET search_path TO ''
AS \$function\$
DECLARE
  decrypted_secret text := (SELECT decrypted_secret FROM vault.decrypted_secrets WHERE id = secret_id);
BEGIN
  UPDATE vault.secrets s
  SET
    secret = CASE WHEN new_secret IS NULL THEN s.secret
                  ELSE encode(pgsodium.crypto_aead_det_encrypt(
                    message := convert_to(new_secret, 'utf8'),
                    additional := convert_to(s.id::text, 'utf8'),
                    key_id := 0::bigint,
                    context := 'pgsodium'::bytea,
                    nonce := s.nonce
                  ), 'base64') END,
    name = coalesce(new_name, s.name),
    description = coalesce(new_description, s.description),
    updated_at = now()
  WHERE s.id = secret_id;
END
\$function\$;
EOF
"

    # Apply the SQL patch
    ssh_exec "sudo -u postgres psql -d postgres -f /tmp/vault_fix.sql && rm /tmp/vault_fix.sql"

    log_success "Vault functions patched to use pgsodium server key"
}

#--------------------------------------------------------------#
# Fix Schema Ownership for Supabase
#--------------------------------------------------------------#
fix_schema_ownership() {
    log_step "Fixing Schema Ownership"

    log_info "Setting correct ownership for auth and storage schemas..."

    ssh_exec "sudo -u postgres psql -d postgres << 'EOF'
-- Fix auth schema ownership (schema, tables, sequences, functions)
ALTER SCHEMA auth OWNER TO supabase_auth_admin;
DO \$\$
DECLARE r RECORD;
BEGIN
    FOR r IN SELECT tablename FROM pg_tables WHERE schemaname = 'auth'
    LOOP EXECUTE 'ALTER TABLE auth.' || quote_ident(r.tablename) || ' OWNER TO supabase_auth_admin'; END LOOP;
    FOR r IN SELECT sequence_name FROM information_schema.sequences WHERE sequence_schema = 'auth'
    LOOP EXECUTE 'ALTER SEQUENCE auth.' || quote_ident(r.sequence_name) || ' OWNER TO supabase_auth_admin'; END LOOP;
    FOR r IN SELECT p.proname, pg_get_function_identity_arguments(p.oid) as args
             FROM pg_proc p JOIN pg_namespace n ON p.pronamespace = n.oid WHERE n.nspname = 'auth'
    LOOP EXECUTE 'ALTER FUNCTION auth.' || quote_ident(r.proname) || '(' || r.args || ') OWNER TO supabase_auth_admin'; END LOOP;
END\$\$;

-- Fix storage schema ownership (schema, tables, sequences, functions)
ALTER SCHEMA storage OWNER TO supabase_storage_admin;
DO \$\$
DECLARE r RECORD;
BEGIN
    FOR r IN SELECT tablename FROM pg_tables WHERE schemaname = 'storage'
    LOOP EXECUTE 'ALTER TABLE storage.' || quote_ident(r.tablename) || ' OWNER TO supabase_storage_admin'; END LOOP;
    FOR r IN SELECT sequence_name FROM information_schema.sequences WHERE sequence_schema = 'storage'
    LOOP EXECUTE 'ALTER SEQUENCE storage.' || quote_ident(r.sequence_name) || ' OWNER TO supabase_storage_admin'; END LOOP;
    FOR r IN SELECT p.proname, pg_get_function_identity_arguments(p.oid) as args
             FROM pg_proc p JOIN pg_namespace n ON p.pronamespace = n.oid WHERE n.nspname = 'storage'
    LOOP EXECUTE 'ALTER FUNCTION storage.' || quote_ident(r.proname) || '(' || r.args || ') OWNER TO supabase_storage_admin'; END LOOP;
END\$\$;

-- Grant full permissions on auth schema
GRANT ALL ON SCHEMA auth TO supabase_auth_admin;
GRANT CREATE ON SCHEMA auth TO supabase_auth_admin;
GRANT ALL ON ALL TABLES IN SCHEMA auth TO supabase_auth_admin;
GRANT ALL ON ALL SEQUENCES IN SCHEMA auth TO supabase_auth_admin;
GRANT ALL ON ALL FUNCTIONS IN SCHEMA auth TO supabase_auth_admin;
ALTER DEFAULT PRIVILEGES IN SCHEMA auth GRANT ALL ON TABLES TO supabase_auth_admin;
ALTER DEFAULT PRIVILEGES IN SCHEMA auth GRANT ALL ON SEQUENCES TO supabase_auth_admin;

-- Grant full permissions on storage schema
GRANT ALL ON SCHEMA storage TO supabase_storage_admin;
GRANT CREATE ON SCHEMA storage TO supabase_storage_admin;
GRANT ALL ON ALL TABLES IN SCHEMA storage TO supabase_storage_admin;
GRANT ALL ON ALL SEQUENCES IN SCHEMA storage TO supabase_storage_admin;
GRANT ALL ON ALL FUNCTIONS IN SCHEMA storage TO supabase_storage_admin;
ALTER DEFAULT PRIVILEGES IN SCHEMA storage GRANT ALL ON TABLES TO supabase_storage_admin;
ALTER DEFAULT PRIVILEGES IN SCHEMA storage GRANT ALL ON SEQUENCES TO supabase_storage_admin;

-- Grant CREATE on public schema (needed for auth migrations)
GRANT CREATE ON SCHEMA public TO supabase_auth_admin;
GRANT CREATE ON SCHEMA public TO supabase_storage_admin;
EOF" 2>/dev/null

    log_success "Schema ownership fixed"
}

#--------------------------------------------------------------#
# Fix Storage Search Path
# Creates views in public schema so storage API can find tables
# when using connection pooling that doesn't respect search_path
#--------------------------------------------------------------#
fix_storage_search_path() {
    log_step "Fixing Storage Search Path"

    log_info "Creating views in public schema for storage tables..."

    ssh_exec "sudo -u postgres psql -d postgres << 'EOF'
-- Create views in public schema that reference storage tables
-- This fixes the issue where connection pooling doesn't apply the role's search_path
CREATE OR REPLACE VIEW public.buckets AS SELECT * FROM storage.buckets;
CREATE OR REPLACE VIEW public.buckets_analytics AS SELECT * FROM storage.buckets_analytics;
CREATE OR REPLACE VIEW public.objects AS SELECT * FROM storage.objects;
CREATE OR REPLACE VIEW public.migrations AS SELECT * FROM storage.migrations;
CREATE OR REPLACE VIEW public.s3_multipart_uploads AS SELECT * FROM storage.s3_multipart_uploads;
CREATE OR REPLACE VIEW public.s3_multipart_uploads_parts AS SELECT * FROM storage.s3_multipart_uploads_parts;
CREATE OR REPLACE VIEW public.prefixes AS SELECT * FROM storage.prefixes;

-- Grant permissions to storage admin
GRANT SELECT, INSERT, UPDATE, DELETE ON public.buckets TO supabase_storage_admin;
GRANT SELECT, INSERT, UPDATE, DELETE ON public.buckets_analytics TO supabase_storage_admin;
GRANT SELECT, INSERT, UPDATE, DELETE ON public.objects TO supabase_storage_admin;
GRANT SELECT, INSERT, UPDATE, DELETE ON public.migrations TO supabase_storage_admin;
GRANT SELECT, INSERT, UPDATE, DELETE ON public.s3_multipart_uploads TO supabase_storage_admin;
GRANT SELECT, INSERT, UPDATE, DELETE ON public.s3_multipart_uploads_parts TO supabase_storage_admin;
GRANT SELECT, INSERT, UPDATE, DELETE ON public.prefixes TO supabase_storage_admin;

-- Also set search_path for the role in this database (belt and suspenders)
ALTER ROLE supabase_storage_admin IN DATABASE postgres SET search_path TO storage, public, extensions;
EOF" 2>/dev/null

    log_success "Storage search path fixed"
}

#--------------------------------------------------------------#
# Launch Supabase
#--------------------------------------------------------------#
launch_supabase() {
    load_env
    log_step "Launching Supabase"

    log_info "Starting Supabase containers (11 services)..."

    ssh_exec "cd ~/pigsty && ./app.yml"

    log_success "Supabase launched"

    # Fix Vault to use pgsodium server key
    fix_vault_pgsodium

    # Fix schema ownership for supabase_auth_admin and supabase_storage_admin
    fix_schema_ownership

    # Fix storage search_path (create views in public schema)
    fix_storage_search_path

    # Restart auth and storage containers to apply ownership changes
    log_info "Restarting auth and storage containers..."
    ssh_exec "cd /opt/supabase && sudo docker compose restart auth storage" || true

    # Wait for containers to start
    log_info "Waiting for services to start..."
    sleep 20

    # Check container status
    log_info "Checking container status..."
    ssh_exec "sudo docker ps --format 'table {{.Names}}\t{{.Status}}' | grep supabase" || true
}

#--------------------------------------------------------------#
# Verify deployment
#--------------------------------------------------------------#
verify_deployment() {
    log_step "Verifying Deployment"

    load_env

    # Check container count
    local container_count=$(ssh_exec "sudo docker ps | grep supabase | wc -l" || echo "0")

    if [ "${container_count}" -eq 11 ]; then
        log_success "All 11 Supabase containers running"
    else
        log_warning "Only ${container_count}/11 containers running"
    fi

    # Check HTTPS
    if [ "${USE_LETSENCRYPT:-false}" = "true" ]; then
        log_info "Testing HTTPS access..."
        if curl -sI "https://${SUPABASE_DOMAIN}" | grep -q "HTTP"; then
            log_success "HTTPS working: https://${SUPABASE_DOMAIN}"
        else
            log_warning "HTTPS may not be working yet"
        fi
    fi

    # Check PostgreSQL
    log_info "PostgreSQL available at: ${VPS_HOST}:5436"

    # Check Grafana
    log_info "Grafana available at: http://${VPS_HOST}"

    echo ""
    print_deployment_summary
}

#--------------------------------------------------------------#
# Print deployment summary
#--------------------------------------------------------------#
print_deployment_summary() {
    load_env

    local protocol="http"
    if [ "${USE_LETSENCRYPT:-false}" = "true" ]; then
        protocol="https"
    fi

    echo ""
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    echo "  Deployment Complete!"
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    echo ""
    echo "Flutter App:"
    echo "   URL:      ${protocol}://${SUPABASE_DOMAIN}"
    echo ""
    echo "Supabase API:"
    echo "   URL:      ${protocol}://api.${SUPABASE_DOMAIN}"
    echo ""
    echo "Supabase Studio:"
    echo "   URL:      ${protocol}://studio.${SUPABASE_DOMAIN}"
    echo "   User:     ${DASHBOARD_USERNAME:-supabase}"
    echo "   Password: ${DASHBOARD_PASSWORD:-pigsty}"
    echo ""
    echo "Grafana:"
    echo "   URL:      http://${VPS_HOST}"
    echo "   User:     admin"
    echo "   Password: ${GRAFANA_ADMIN_PASSWORD}"
    echo ""
    echo "PostgreSQL:"
    echo "   Host:     ${VPS_HOST}:5436"
    echo "   Database: postgres"
    echo "   User:     supabase_admin"
    echo "   Password: ${POSTGRES_PASSWORD}"
    echo ""
    if [ -n "${S3_BUCKET:-}" ]; then
        echo "Storage (S3):"
        echo "   Bucket:   ${S3_BUCKET}"
        echo "   Endpoint: ${S3_ENDPOINT}"
        echo "   Status:   Configured"
        echo ""
    fi
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
}

#--------------------------------------------------------------#
# Configure Multi-Domain Architecture
#--------------------------------------------------------------#
configure_domains() {
    log_step "Configuring Multi-Domain Architecture"
    "${SCRIPT_DIR}/modules/13-configure-domains.sh"
}

#--------------------------------------------------------------#
# Sync All: Schema + Flutter to Staging
#--------------------------------------------------------------#
sync_all_to_staging() {
    print_banner

    local ORIGEN_IP="${ORIGEN_IP:-194.163.149.70}"
    local STAGING_IP="${VPS_HOST:-167.114.2.209}"

    echo ""
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    echo "  SYNC ALL TO STAGING"
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    echo ""
    echo "  Origen:  ${ORIGEN_IP}"
    echo "  Staging: ${STAGING_IP}"
    echo ""

    # Step 1: Sync schema from origen
    log_step "Step 1/3: Syncing schema from origen"
    "${SCRIPT_DIR}/modules/14-sync-app-schema.sh" "${ORIGEN_IP}"

    # Step 2: Apply schema to staging
    log_step "Step 2/3: Applying schema to staging"
    "${SCRIPT_DIR}/modules/15-apply-app-schema.sh" "${STAGING_IP}"

    # Step 3: Build and deploy Flutter
    log_step "Step 3/3: Building and deploying Flutter Web"
    deploy_flutter_web

    echo ""
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    echo "  SYNC COMPLETE!"
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    echo ""
    echo "  Schema:  Synced from ${ORIGEN_IP} -> ${STAGING_IP}"
    echo "  Flutter: Deployed to https://${SUPABASE_DOMAIN}"
    echo "  API:     https://api.${SUPABASE_DOMAIN}"
    echo ""
}

#--------------------------------------------------------------#
# Deploy Flutter Web Application
#--------------------------------------------------------------#
deploy_flutter() {
    log_step "Deploying Flutter Web Application"
    "${SCRIPT_DIR}/modules/12-deploy-flutter-web.sh"
}

#--------------------------------------------------------------#
# Full deployment
#--------------------------------------------------------------#
deploy_all() {
    print_banner

    load_env

    log_info "Starting Pigsty + Supabase deployment..."
    log_info "Target: ${VPS_HOST}"
    log_info "Domain: ${SUPABASE_DOMAIN:-not set}"
    echo ""

    # Phase 1: Prepare VPS and download Pigsty
    prepare_vps
    download_pigsty
    update_supabase_versions
    generate_pigsty_config
    bootstrap_ansible

    # Validate configuration before proceeding
    "${SCRIPT_DIR}/modules/03-validate.sh"

    # Phase 2: Wait for apt locks (Ubuntu automatic updates on fresh install)
    log_step "Waiting for system to be ready"
    log_info "Checking for apt locks (unattended-upgrades may be running on fresh Ubuntu)..."
    ssh_exec "
        max_wait=600  # 10 minutes max
        wait_time=0
        interval=10

        while sudo fuser /var/lib/dpkg/lock-frontend >/dev/null 2>&1 || \
              sudo fuser /var/lib/dpkg/lock >/dev/null 2>&1 || \
              sudo fuser /var/lib/apt/lists/lock >/dev/null 2>&1; do

            if [ \$wait_time -ge \$max_wait ]; then
                echo 'ERROR: apt lock still held after 10 minutes'
                exit 1
            fi

            echo \"Waiting for apt locks... (\${wait_time}s / \${max_wait}s)\"
            sleep \$interval
            wait_time=\$((wait_time + interval))
        done

        echo 'System ready - apt locks released'
    "
    log_success "System ready for installation"

    # Phase 3: Install complete Pigsty stack
    # Use install.yml which runs: node.yml (configures repos) -> infra.yml -> pgsql.yml
    log_step "Installing Pigsty Stack (this takes 15-25 minutes)"

    log_info "Running install.yml - this configures repos, installs PostgreSQL, etcd, MinIO, nginx, Docker..."
    ssh_exec "cd ~/pigsty && ./install.yml" || {
        log_warning "install.yml had some warnings, checking status..."
    }

    # Phase 3: Install Docker (required before app.yml)
    # According to Pigsty docs: docker.yml must run AFTER install.yml and BEFORE app.yml
    install_docker

    # Phase 4: Fix nginx SSL config if needed (single domain for cert)
    log_info "Fixing nginx SSL configuration..."
    ssh_exec "
        # Ensure cert directory exists
        sudo mkdir -p /etc/nginx/conf.d/cert

        # Generate self-signed cert if not exists
        if [ ! -f /etc/nginx/conf.d/cert/${SUPABASE_DOMAIN}.crt ]; then
            sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
                -keyout /etc/nginx/conf.d/cert/${SUPABASE_DOMAIN}.key \
                -out /etc/nginx/conf.d/cert/${SUPABASE_DOMAIN}.crt \
                -subj '/CN=${SUPABASE_DOMAIN}' 2>/dev/null
        fi

        # Fix supa.conf if it has multiple domains in ssl_certificate path
        if grep -q 'ssl_certificate.*api\.' /etc/nginx/conf.d/supa.conf 2>/dev/null; then
            sudo sed -i 's|ssl_certificate.*/etc/nginx/conf.d/cert/api\..*\.crt;|ssl_certificate /etc/nginx/conf.d/cert/${SUPABASE_DOMAIN}.crt;|' /etc/nginx/conf.d/supa.conf
            sudo sed -i 's|ssl_certificate_key.*/etc/nginx/conf.d/cert/api\..*\.key;|ssl_certificate_key /etc/nginx/conf.d/cert/${SUPABASE_DOMAIN}.key;|' /etc/nginx/conf.d/supa.conf
        fi

        # Test and start nginx
        sudo nginx -t && sudo systemctl restart nginx || true
    " || log_warning "Nginx fix had warnings"

    # Phase 5: Launch Supabase containers (app.yml)
    launch_supabase

    # Phase 5: Configure pgBackRest for Backblaze B2
    if [ "${PGBACKREST_ENABLED:-false}" = "true" ]; then
        configure_pgbackrest_b2
    fi

    # Phase 6: Restore database backups if available
    if [ -d "${PROJECT_ROOT}/db_backup" ] && [ -f "${PROJECT_ROOT}/db_backup/postgres.dump" ]; then
        log_info "Database backups found, will restore after deployment..."
        "${SCRIPT_DIR}/modules/10-restore-backup.sh"
    fi

    # Phase 7: Configure multi-domain architecture + SSL
    configure_domains

    # Phase 8: Deploy Flutter Web if project is configured
    if [ -n "${FLUTTER_PROJECT_PATH:-}" ]; then
        if [ -d "${FLUTTER_PROJECT_PATH}" ]; then
            log_info "Flutter project found at ${FLUTTER_PROJECT_PATH}, deploying web app..."
            deploy_flutter
        else
            log_warning "FLUTTER_PROJECT_PATH set but directory not found: ${FLUTTER_PROJECT_PATH}"
        fi
    fi

    # Phase 9: Security hardening (firewall, fail2ban, SSH)
    configure_security

    # Phase 10: Health monitoring setup
    configure_health_monitoring

    verify_deployment

    log_success "Deployment completed successfully!"
}

#--------------------------------------------------------------#
# Security Hardening
#--------------------------------------------------------------#
configure_security() {
    log_step "Security Hardening"
    "${SCRIPT_DIR}/modules/16-security-hardening.sh"
}

#--------------------------------------------------------------#
# Health Monitoring Setup
#--------------------------------------------------------------#
configure_health_monitoring() {
    log_step "Health Monitoring"
    "${SCRIPT_DIR}/modules/17-health-check.sh"
}

#--------------------------------------------------------------#
# Configure pgBackRest for Backblaze B2
#--------------------------------------------------------------#
configure_pgbackrest_b2() {
    load_env
    log_step "Configuring pgBackRest for Backblaze B2"

    # Check if B2 credentials are configured
    if [ -z "${PGBACKREST_S3_BUCKET:-}" ] || [ -z "${PGBACKREST_S3_ACCESS_KEY:-}" ]; then
        log_warning "Backblaze B2 not configured in .env, skipping pgBackRest setup"
        return 0
    fi

    log_info "Setting up pgBackRest with Backblaze B2..."

    # Create pgbackrest config for B2
    ssh_exec "sudo tee /etc/pgbackrest/pgbackrest.conf > /dev/null << 'EOF'
#==============================================================#
# pgBackRest Configuration - Backblaze B2
#==============================================================#

[pg-meta]
pg1-path=/pg/data
pg1-port=5432

[global]
# Archive settings
archive-async=y

# Backup settings
annotation=pg_cluster=pg-meta
start-fast=y
expire-auto=y

# Compression
compress-type=lz4
spool-path=/pg/spool

# Logging
log-level-console=info
log-level-file=info
log-path=/pg/log/pgbackrest

# Backblaze B2 Repository (S3-compatible)
repo1-type=s3
repo1-s3-endpoint=${PGBACKREST_S3_ENDPOINT#https://}
repo1-s3-region=${PGBACKREST_S3_REGION}
repo1-s3-bucket=${PGBACKREST_S3_BUCKET}
repo1-s3-key=${PGBACKREST_S3_ACCESS_KEY}
repo1-s3-key-secret=${PGBACKREST_S3_SECRET_KEY}
repo1-s3-uri-style=host
repo1-path=/pgbackrest/${VPS_HOST}
repo1-block=y
repo1-bundle=y
repo1-bundle-limit=20MiB
repo1-bundle-size=128MiB
repo1-cipher-type=aes-256-cbc
repo1-cipher-pass=${PGBACKREST_CIPHER_PASS}
repo1-retention-full-type=time
repo1-retention-full=${PGBACKREST_RETENTION_FULL:-14}

# Restore settings
delta=y
archive-mode=off

# Parallel processing
[global:restore]
process-max=8

[global:archive-push]
process-max=4

[global:archive-get]
process-max=2
EOF"

    # Create stanza
    log_info "Creating pgBackRest stanza..."
    ssh_exec "sudo -u postgres pgbackrest --stanza=pg-meta stanza-create" || {
        log_warning "Stanza creation failed (may already exist)"
    }

    # Verify stanza
    log_info "Verifying pgBackRest configuration..."
    if ssh_exec "sudo -u postgres pgbackrest --stanza=pg-meta check" 2>/dev/null; then
        log_success "pgBackRest configured and working with Backblaze B2"
    else
        log_warning "pgBackRest check failed - may need manual verification"
    fi

    # Run initial backup
    log_info "Running initial full backup to Backblaze B2..."
    ssh_exec "sudo -u postgres pgbackrest --stanza=pg-meta --type=full backup" && {
        log_success "Initial backup completed successfully"
    } || {
        log_warning "Initial backup failed - check logs at /pg/log/pgbackrest/"
    }

    # Configure automatic daily backups
    log_info "Configuring automatic daily backups at 2 AM..."
    ssh_exec "sudo -u postgres crontab -l 2>/dev/null | grep -v pgbackrest > /tmp/postgres_cron || true
echo '# pgBackRest automatic backup to Backblaze B2' >> /tmp/postgres_cron
echo '0 2 * * * /usr/bin/pgbackrest --stanza=pg-meta --type=full backup >> /pg/log/pgbackrest/cron.log 2>&1' >> /tmp/postgres_cron
sudo -u postgres crontab /tmp/postgres_cron
rm /tmp/postgres_cron"

    log_success "Automatic daily backups configured"

    # Show backup info
    log_info "pgBackRest status:"
    ssh_exec "sudo -u postgres pgbackrest --stanza=pg-meta info" || true
}

#--------------------------------------------------------------#
# Check and run interactive setup if needed
#--------------------------------------------------------------#
check_env_or_setup() {
    cd "${PROJECT_ROOT}"

    if [ ! -f ".env" ]; then
        echo ""
        echo -e "${YELLOW}No .env file found. Starting interactive setup...${NC}"
        echo ""
        "${SCRIPT_DIR}/setup-interactive.sh"

        # Verify .env was created
        if [ ! -f ".env" ]; then
            log_error "Setup was cancelled or failed. Cannot continue."
            exit 1
        fi
    fi
}

#--------------------------------------------------------------#
# Main
#--------------------------------------------------------------#
main() {
    case "${1:-help}" in
        all)
            check_env_or_setup
            deploy_all
            ;;
        setup)
            "${SCRIPT_DIR}/setup-interactive.sh"
            ;;
        config)
            load_env
            generate_pigsty_config
            ;;
        validate)
            "${SCRIPT_DIR}/modules/03-validate.sh"
            ;;
        verify)
            verify_deployment
            ;;
        flutter)
            load_env
            deploy_flutter
            ;;
        domains)
            load_env
            configure_domains
            ;;
        backup)
            "${SCRIPT_DIR}/modules/11-create-backup.sh"
            ;;
        restore)
            "${SCRIPT_DIR}/modules/10-restore-backup.sh"
            ;;
        sync-schema)
            "${SCRIPT_DIR}/modules/14-sync-app-schema.sh" "${2:-}"
            ;;
        apply-schema)
            load_env
            "${SCRIPT_DIR}/modules/15-apply-app-schema.sh" "${2:-${VPS_HOST}}"
            ;;
        sync-all)
            load_env
            sync_all_to_staging
            ;;
        security)
            load_env
            configure_security
            ;;
        health)
            load_env
            configure_health_monitoring
            ;;
        *)
            print_banner
            echo "Usage: $0 {all|setup|config|validate|verify|domains|flutter|backup|restore|security|health}"
            echo ""
            echo "Commands:"
            echo "  all          - Full deployment (runs setup if no .env)"
            echo "  setup        - Interactive configuration wizard"
            echo "  config       - Generate and upload pigsty.yml only"
            echo "  validate     - Validate pigsty.yml configuration"
            echo "  verify       - Verify deployment status"
            echo "  domains      - Configure multi-domain Nginx + SSL"
            echo "  flutter      - Build and deploy Flutter Web app"
            echo "  backup       - Create database backup from VPS"
            echo "  restore      - Restore database backup to VPS"
            echo "  security     - Apply security hardening (firewall, fail2ban, SSH)"
            echo "  health       - Setup health monitoring (cron + HTTP endpoint)"
            echo ""
            echo "Quick Start:"
            echo "  $0 all       # Interactive setup + full deployment"
            echo ""
            echo "After deployment, your URLs will be:"
            echo "  https://yourdomain.com       -> Flutter App"
            echo "  https://api.yourdomain.com   -> Supabase API"
            echo "  https://studio.yourdomain.com -> Supabase Studio"
            echo ""
            echo "Features:"
            echo "  • PostgreSQL 18 with Patroni HA"
            echo "  • Supabase (Auth, Storage, Realtime, Edge Functions)"
            echo "  • Automatic backups to Backblaze B2"
            echo "  • Let's Encrypt SSL certificates"
            echo "  • Grafana monitoring"
            ;;
    esac
}

main "$@"
